import ::cuda;

func init(){
    cuda::cuInit(0i32);
}

func setDevice(id: i64){
    cuda::cudaSetDevice(id).tostr();
}

func maxThreadPerBlock(device: i64)->i64{
    var count: i32;
    cuda::cuDeviceGetAttribute(&count, 1i32, device.toi32());
    return count.toi64();
}

func multiProcessorsCount(device: i64)->i64{
    var count: i32;
    cuda::cuDeviceGetAttribute(&count, 16i32, device.toi32());
    return count.toi64();
}

func launchKernel(kernel_function_pointer: i8*, gridShape: Array<i64, 3>, blockShape: Array<i64, 3>, kernel_params: i8**)->i64{
    var re = cuda::cuLaunchKernel(kernel_function_pointer, gridShape[0].tou32(), gridShape[1].tou32(),
    gridShape[2].tou32(), blockShape[0].tou32(), blockShape[1].tou32(), blockShape[2].tou32(), 0u32, i64_to_rawptr<i8>(0), kernel_params, i64_to_rawptr<i8*>(0));
    return re.toi64();
}

func launchKernelForDim1(kernel_function_pointer: i8*, paramters: i8**){

}

template <Type_, Dim_>
struct Tensor<Type_, Dim_> {
    data : Type_*;
    layout: Layout<Dim_>;
}

template <Type_, Dim_>
interface atProperty {
    func get(idx: Array<i64, Dim_>)->Type_;
    func set(idx: Array<i64, Dim_>, value: Type_);
}

template <Type_, Dim_>
implement atProperty<Type_, Dim_> for Tensor<Type_, Dim_>{
    func get(idx: Array<i64, Dim_>)->Type_{
        var offset = this.layout.arrayIndexToLinearIndex(idx);
        return this.data[offset];
    }

    func set(idx: Array<i64, Dim_>, value: Type_){
        var offset = this.layout.arrayIndexToLinearIndex(idx);
        this.data[offset] = value;
    }
}

template <Type_, Dim_>
implement operator::ArrayIndex<Type_, Dim_> for Tensor<Type_, Dim_> {
    func get(index: Array<i64, Dim_>)->Type_{
        return this.at(index);
    }

    func set(index: Array<i64, Dim_>, value: Type_){
        this.at(index) = value;
    }
}

template <Type_, Dim_>
implement Tensor<Type_, Dim_> {
    @static
    func create(shape :Array<i64, Dim_>)->Tensor<Type_, Dim_>{
        var self: Tensor<Type_, Dim_>;
        self.layout = Layout<Dim_>::create(shape);

        var bytes = self.layout.length() * sizeof(Type_);
        cuda::cuMemAlloc(__bit_cast<Type_** ,i8**>(&self.data), bytes);
        bindings::registerReferenceCount(__bit_cast<Type_* ,undef*>(self.data));
        bindings::incrementReferenceCount(__bit_cast<Type_* ,undef*>(self.data));

        return self;
    }

    func toHost()->::Tensor<Type_, Dim_>{
        var host_tensor = ::Tensor<Type_, Dim_>::create(this.layout.shape);
        var size = 1;
        for i in 0 to Dim_ {
            size = size * this.layout.shape[i];
        }
        var re =  cuda::cudaMemcpy(__bit_cast<Type_* ,i8*>(host_tensor.data.raw_ptr), __bit_cast<Type_*, i8*>(this.data), size * sizeof(Type_), 2i32);
        return host_tensor;
    }
}

template <Type_, Dim_>
implement operator::ArrayIndex<Type_, Dim_> for Tensor<Type_, Dim_> {
    func get(idx: Array<i64, Dim_>)->Type_{
        return this.at(idx);
    }

    func set(idx: Array<i64, Dim_>, value: Type_){
        this.at(idx) = value;
    }
}

template <Type_, Dim_>
implement ::Tensor<Type_, Dim_> {
    func toGpu()->Tensor<Type_, Dim_>{
        var cuda_tensor = Tensor<Type_, Dim_>::create(this.layout.shape);
        var size = 1;
        for i in 0 to Dim_ {
            size = size * this.layout.shape[i];
        }
        var re = cuda::cudaMemcpy(__bit_cast<Type_* ,i8*>(cuda_tensor.data), __bit_cast<Type_*, i8*>(this.data.raw_ptr), size * sizeof(Type_), 1i32);
        return cuda_tensor;
    }
}

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.tid.x")
func __thread_idx_x()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.tid.y")
func __thread_idx_y()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.tid.z")
func __thread_idx_z()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.ntid.x")
func __block_shape_x()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.ntid.y")
func __block_shape_y()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.ntid.z")
func __block_shape_z()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.ctaid.x")
func __block_idx_x()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.ctaid.y")
func __block_idx_y()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.ctaid.z")
func __block_idx_z()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.nctaid.x")
func __grid_shape_x()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.nctaid.y")
func __grid_shape_y()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.nctaid.z")
func __grid_shape_z()->u32;

@target("nvptx")
@intrinsic("llvm.nvvm.read.ptx.sreg.warpsize")
func __warp_size()->u32;

@target("nvptx")
func threadIndex()->Array<i64, 3> {
    return [__thread_idx_x().toi64(), __thread_idx_y().toi64(), __thread_idx_z().toi64()];
}

@target("nvptx")
func blockShape()->Array<i64, 3> {
    return [__block_shape_x().toi64(), __block_shape_y().toi64(), __block_shape_z().toi64()];
}

@target("nvptx")
func blockIndex()->Array<i64, 3> {
    return [__block_idx_x().toi64(), __block_idx_y().toi64(), __block_idx_z().toi64()];
}

@target("nvptx")
func gridShape()->Array<i64, 3> {
    return [__grid_shape_x().toi64(), __grid_shape_y().toi64(), __grid_shape_z().toi64()];
}
